# -*- coding: utf-8 -*-
"""AI_DEVCLUB_IITD_TWITTER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fNpZj7dzKZMHYEcbfDkvbJkBqs0QzyBn
"""

import pandas as pd
import numpy as np

df = pd.read_csv("behaviour_simulation_train.csv", engine='python')  # replace with your actual path

print("shape",df.shape)
print("tail",df.tail())
print("info",df.info())
print("nullvalues",df.isnull().sum())

df['likes'] = np.log(df['likes'] + 1)

import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df['likes'], kde=True)
sns.boxplot(x=df['likes'])



df['tweets'] = df['tweets'].str.lower()

print("lowering_values",df['tweets'].head())





print("describe",df['likes'].describe())





word_dict = df['tweets'].str.split().explode().value_counts().to_dict()



print("shape",df.shape)
print("tail",df.tail())
print("info",df.info())
print("nullvalues",df.isnull().sum())

#df.dropna(subset=['content', 'username', 'company', 'likes'], inplace=True)
df['media'] = df['media'].fillna('0')
df['has_media'] = df['media'].apply(lambda x: x != 'no_media')

df['datetime'] = pd.to_datetime(df['date'], errors='coerce')

print("tail",df.tail())


#lowercased, no media term added

df['word_count'] = df['tweets'].apply(lambda x: len(x.split()))
df['char_count'] = df['tweets'].apply(len)

print(df['word_count'])
print(df["char_count"])



"""#TD-IDF"""

import pandas as pd
import numpy as np
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
import math



def calculate_sklearn_tfidf(df, like_thresholds=[1]):
    """
    Calculate TF-IDF using sklearn's TfidfVectorizer
    """
    results = {}

    for threshold in like_thresholds:
        # Filter tweets with likes > threshold
        filtered_df = df[df['likes'] > threshold]

        if len(filtered_df) == 0:
            results[f'likes_>{threshold}'] = {}
            continue

        tweets = filtered_df['tweets'].dropna().tolist()

        if len(tweets) == 0:
            results[f'likes_>{threshold}'] = {}
            continue

        # Use TfidfVectorizer with log scaling
        vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words=None,  # Set to 'english' if you want to remove stop words
            use_idf=True,
            smooth_idf=True,
            sublinear_tf=True  # This applies log scaling to TF
        )

        # Fit and transform the tweets
        tfidf_matrix = vectorizer.fit_transform(tweets)
        feature_names = vectorizer.get_feature_names_out()

        # Calculate average TF-IDF scores across all documents
        avg_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)

        # Create dictionary with word: avg_tfidf_score
        word_tfidf = {}
        for i, word in enumerate(feature_names):
            word_tfidf[word] = {
                'avg_tfidf': avg_tfidf_scores[i],
                'idf': vectorizer.idf_[i]
            }

        results[f'likes_>{threshold}'] = word_tfidf

    return results

def display_top_tfidf_words(tfidf_results, top_n=10):
    """
    Display top N words by TF-IDF score for each threshold
    """
    for threshold, words in tfidf_results.items():
        print(f"\n{threshold} - Top {top_n} words by TF-IDF:")
        print("-" * 50)

        if not words:
            print("No data available")
            continue

        # Sort by average TF-IDF score
        sorted_words = sorted(words.items(),
                            key=lambda x: x[1]['avg_tfidf'],
                            reverse=True)[:top_n]

        for word, scores in sorted_words:
          print(f"{word:<15} TF-IDF: {scores['avg_tfidf']:.4f}")

def compare_words_across_thresholds(tfidf_results, words_to_compare):
    """
    Compare specific words across different like thresholds
    """
    print(f"\nComparison of words across thresholds:")
    print("-" * 60)

    # Create header
    thresholds = list(tfidf_results.keys())
    header = f"{'Word':<15}"
    for threshold in thresholds:
        header += f"{threshold:<15}"
    print(header)
    print("-" * 60)

    for word in words_to_compare:
        row = f"{word:<15}"
        for threshold in thresholds:
            if word in tfidf_results[threshold]:
                score = tfidf_results[threshold][word]['avg_tfidf']
                row += f"{score:<15.4f}"
            else:
                row += f"{'N/A':<15}"
        print(row)

# Example usage


sklearn_results = calculate_sklearn_tfidf(df)
display_top_tfidf_words(sklearn_results, top_n=1000)

    # Compare specific words



    # Calculate TF-IDF manually (for understanding)
print("\n" + "="*60)
print("Manual TF-IDF Calculation (for verification)")
print("="*60)

"""playing with text blob


"""

from textblob import TextBlob

word = "i think the movie is very very bad and is a disaster"
TextBlob(word).sentiment.polarity

df['sentiment'] = df['tweets'].apply(lambda tweet: 1000*TextBlob(tweet).sentiment.polarity)

print(df['sentiment'])

print(df['sentiment'].describe())

print(df.describe())

import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
import numpy as np


def plot_pdf(df, feature_name):
    """
    Plots the probability density function for a given feature in a DataFrame.

    Args:
        df (pd.DataFrame): The input DataFrame.
        feature_name (str): The name of the feature to plot.
    """
    data = df[feature_name].dropna()  # Extract data, removing NaN values
    if len(data) == 0:
        print("No valid data for the selected feature.")
        return

    kde = gaussian_kde(data)  # Create a KDE object
    xs = np.linspace(min(data), max(data), 1000)  # Generate x values for plotting
    density = kde(xs)  # Compute density values

    plt.figure(figsize=(10, 6))
    plt.plot(xs, density)
    plt.title(f'Probability Density Function of {feature_name}')
    plt.xlabel(feature_name)
    plt.ylabel('Density')
    plt.grid(True)
    plt.show()


# Example Usage
# Assuming your DataFrame is named 'df' and your feature is 'my_feature'
# Create a sample DataFrame for demonstration


plot_pdf(df, 'sentiment')

# Create the scatter plot
plt.figure(figsize=(10, 6)) # Adjust figure size if needed
plt.scatter(df['sentiment'], df['likes'])

# Add labels and title
plt.xlabel('sentiment')
plt.ylabel('likes')
plt.title('Scatter Plot of Feature 1 vs Feature 2')

# Display the plot
plt.grid(True) # Add grid lines
plt.show()

df_f = df[df['likes'] < 40000]
plt.figure(figsize=(10, 6)) # Adjust figure size if needed
plt.plot(df_f['sentiment'], df_f['likes'])

# Add labels and title
plt.xlabel('sentiment')
plt.ylabel('likes')
plt.title('Scatter Plot of Feature 1 vs Feature 2')

# Display the plot
plt.grid(True) # Add grid lines
plt.show()

# Create the scatter plot
plt.figure(figsize=(10, 6)) # Adjust figure size if needed
plt.plot(df['word_count'], df['char_count'])

# Add labels and title
plt.xlabel('word_count')
plt.ylabel('char_count')
plt.title('Scatter Plot of Feature 1 vs Feature 2')

# Display the plot
plt.grid(True) # Add grid lines
plt.show()

"""THE PLOT OF SENTIMENT VS LIKES SEEMS LIKE GAUSSIAN

"""

df_f = df[df['likes'] < 1000000]
plt.figure(figsize=(10, 6)) # Adjust figure size if needed
plt.plot(df_f['word_count'], df_f['likes'])

# Add labels and title
plt.xlabel('word_count')
plt.ylabel('likes')
plt.title('Scatter Plot of Feature 1 vs Feature 2')

# Display the plot
plt.grid(True) # Add grid lines
plt.show()

"""#Encoding the company names to digits"""

from sklearn.preprocessing import LabelEncoder


le = LabelEncoder()


df['company_encoded'] = le.fit_transform(df['inferred company'])

encoding_map = {cls: idx for idx, cls in enumerate(le.classes_)}
print(df[['inferred company', 'company_encoded']].head())
print(encoding_map)

print(df['has_media'])

df['has_media'] = df['has_media'].apply(lambda x: 1 if x else 0)

print(df.head())

df['company_avg_likes'] = df.groupby('inferred company')['likes'].transform('mean')

# Calculate company-specific average likes
company_avg_likes = df.groupby('inferred company')['likes'].mean()

# Convert to dictionary mapping
company_to_avg_likes = company_avg_likes.to_dict()

df['hashtag'] = df['tweets'].apply(lambda x: 1 if '#' in str(x) else 0)

avg_likes = df['likes'].mean()
df['likes_adjusted'] = df['likes'].apply(lambda x: avg_likes if x > 50000 else x)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import joblib

X = df[['word_count', 'char_count', 'has_media', 'sentiment','company_avg_likes','hashtag']]
y = df['likes_adjusted']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

model = RandomForestRegressor()
model.fit(X_train, y_train)

preds = model.predict(X_test)
rmse = mean_squared_error(y_test, preds)
print("RMSE:", np.sqrt(rmse))

"""ADDING AVERAGE LIKES BY COMPANY"""

print(df['company_avg_likes'])

"""HASHTAG FEATURE"""

print(df['hashtag'])



"""#MORE DATA ANALYSIS ABOUT LIKES DATA"""

print(df['likes'].describe())

"""ADDING HIGHEST TD IDF WORK OF THAT TWEET AS KEY WORD OF THAT TWEET AND SEE WHAT HAPPENS

"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Create TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
tfidf_matrix = vectorizer.fit_transform(df['tweets'])

# Get feature names (all words)
feature_names = vectorizer.get_feature_names_out()

# Function to get highest TF-IDF word for each tweet
def get_top_tfidf_word(row_index):
    # Get TF-IDF scores for this tweet
    scores = tfidf_matrix[row_index].toarray()[0]

    # Find index of highest score
    top_index = np.argmax(scores)

    # Return the word with highest score
    return feature_names[top_index]

# Create the new column
df['top_keyword'] = [get_top_tfidf_word(i) for i in range(len(df))]

print(df['top_keyword'])

"""ADDIGN AVERAG LIKES PER KEYWORD JUST LIKE AVERAGE LIKES PER COMPANY"""

# Calculate average likes for each keyword
keyword_avg_likes = df.groupby('top_keyword')['likes'].mean()

# Map the average likes back to each tweet based on its keyword
df['avg_likes_for_keyword'] = df['top_keyword'].map(keyword_avg_likes)

keyword_to_avg_likes = keyword_avg_likes.to_dict()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import joblib

X = df[['word_count', 'char_count', 'has_media', 'sentiment','company_avg_likes','hashtag','avg_likes_for_keyword']]
y = df['likes']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

model = RandomForestRegressor()
model.fit(X_train, y_train)

preds = model.predict(X_test)
rmse = mean_squared_error(y_test, preds)
print("RMSE:", np.sqrt(rmse))

import joblib
joblib.dump(model, 'like_predictor.pkl')

'''from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)
model = joblib.load('like_predictor.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    features = np.array([
        data['word_count'],
        data['char_count'],
        data['sentiment'],
        data['company_avg_likes'],
        data['hashtag'],
        data['avg_likes_for_keyword']
    ]).reshape(1, -1)

    prediction = model.predict(features)[0]
    return jsonify({'predicted_likes': int(prediction)})

if __name__ == '__main__':
    app.run(debug=True)'''

'''from flask import Flask
from flask_ngrok import run_with_ngrok
app = Flask(__name__)
run_with_ngrok(app)

@app.route("/")
def home():
    return "<h1>GFG is great platform to learn</h1>"

app.run()'''

print("\nPredicted values:", preds)
print("Actual values:", y_test.values)

preds_exp = np.exp(preds)
y_test_exp = np.exp(y_test)

diff = preds_exp - y_test_exp.values


count_p = 0

count_n = 0



for a in diff:
  if a>0:
    count_p+= 1

  else:
    count_n+=1

print(count_p)
print(count_n)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import joblib
import numpy as np

X = df[['word_count', 'char_count', 'has_media', 'sentiment','company_avg_likes','hashtag','avg_likes_for_keyword']]
y = df['likes']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

model = RandomForestRegressor()
model.fit(X_train, y_train)

preds = model.predict(X_test)

# Apply e^x to both predictions and actual values
preds_exp = np.exp(preds)
y_test_exp = np.exp(y_test)

rmse = mean_squared_error(y_test_exp, preds_exp)
print("RMSE:", np.sqrt(rmse))

# Print predictions vs actual values (exponential transformed)
print("\nPredictions vs Actual (e^x applied):")
print("Predicted\tActual")
print("-" * 20)
for i in range(len(preds_exp)):
    print(f"{preds_exp[i]:.2f}\t\t{y_test_exp.iloc[i]:.2f}")



"""I HAVE ANALYZED THAT THIS MODEL PREDICTS PRETTY WELL IN RANGES

SO DEFINING RANGES

1-50

50 - 100

100 - 1000

1000 - 5000

5000 - 15000

15000 - 50000

50000 - 1000000000000
"""

import numpy as np

preds_exp = np.exp(preds)
y_test_exp = np.exp(y_test)

# Define ranges
def get_range_category(value):
    if 1 <= value < 100:
        return 1
    elif 100 <= value < 1000:
        return 2
    elif 1000 <= value < 5000:
        return 3
    elif 5000 <= value < 20000:
        return 4
    elif 20000 <= value <100000:
        return 5

    else:
        return 0

count = 0

for i in range(len(preds_exp)):
    pred_range = get_range_category(preds_exp[i])
    actual_range = get_range_category(y_test_exp.iloc[i])

    if pred_range != actual_range:
        count += 1

print(f"Number of range mismatches: {count}")
print(f"Total predictions: {len(preds_exp)}")
print(f"Range accuracy: {(len(preds_exp) - count)/len(preds_exp)*100:.1f}%")

from sklearn.metrics import mean_absolute_error
import numpy as np

preds_exp = np.exp(preds)
y_test_exp = np.exp(y_test)


mask = y_test_exp < 10000
filtered_preds = preds_exp[mask]
filtered_actual = y_test_exp[mask]


mae_filtered = mean_absolute_error(filtered_actual, filtered_preds)

print(np.mean(filtered_actual))
print(np.mean(filtered_preds))

print(f"MAE for likes < 10000: {mae_filtered:.2f}")
print(f"Number of samples with likes < 10000: {len(filtered_actual)}")
print(f"Total samples: {len(y_test_exp)}")

"""TILL NOW WE APLLIED ONLY BASIC ML MODELS AND MATH

NOW WE WILL TRY TO USE BERT MODEL WHICH IS ALSO PRETRAINED
"""

'''from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Check for GPU and set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Load BERT and move to GPU
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')
bert_model.to(device)  # Move model to GPU

def get_bert_embeddings_batch_gpu(texts, batch_size=32):  # Increased batch size for GPU
    embeddings = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]

        # Tokenize batch and move to GPU
        inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True,
                          padding=True, max_length=128)

        # Move inputs to GPU
        inputs = {key: value.to(device) for key, value in inputs.items()}

        # Get embeddings for batch
        with torch.no_grad():
            outputs = bert_model(**inputs)
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move back to CPU
            embeddings.extend(batch_embeddings)

    return np.array(embeddings)

# Get BERT features using GPU batch processing
print("Getting BERT embeddings on GPU...")
bert_features = get_bert_embeddings_batch_gpu(df['tweets'].tolist(), batch_size=32)

# Combine with existing features
X_combined = np.concatenate([
    bert_features,  # BERT embeddings (768 dimensions)
    df[['word_count', 'char_count', 'has_media', 'sentiment',
        'company_avg_likes', 'hashtag', 'avg_likes_for_keyword']].values
], axis=1)

# Use with RandomForest
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.1, random_state=42)

print("Training model...")
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)

# Make predictions and calculate RMSE
preds = rf_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print(f"RMSE: {rmse}")'''

from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import pandas as pd

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')
bert_model.to(device)

def get_bert_embeddings_batch_gpu(texts, batch_size=32):
    embeddings = []
    bert_model.eval()

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=128)
        inputs = {key: value.to(device) for key, value in inputs.items()}

        with torch.no_grad():
            outputs = bert_model(**inputs)
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.extend(batch_embeddings)

    return np.array(embeddings)

total_samples_needed = 17000

if len(df) < total_samples_needed:
    print(f"HELLO")
    print("HELLO")
    df_sampled = df.copy()
    y_sampled = y.copy()
else:
    available_indices = df.index.values
    sample_indices = np.random.choice(available_indices, total_samples_needed, replace=False)
    df_sampled = df.loc[sample_indices].reset_index(drop=True)
    y_sampled = y[sample_indices]

print(f"Using {len(df_sampled)} samples for BERT embeddings and model training/testing.")

print("GETTING EMB ON GPU (BERT)")
bert_features = get_bert_embeddings_batch_gpu(df_sampled['tweets'].tolist(), batch_size=32)

X_combined = np.concatenate([
    bert_features,
    df_sampled[['word_count', 'char_count', 'has_media', 'sentiment',
                'company_avg_likes', 'hashtag', 'avg_likes_for_keyword']].values
], axis=1)

X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y_sampled, test_size=500, train_size=16500, random_state=42
)

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

print("Training model...")
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)









preds = rf_model.predict(X_test)





preds_exp = np.exp(preds)
y_test_exp = np.exp(y_test)

rmse = mean_squared_error(y_test_exp, preds_exp)
print("RMSE:", np.sqrt(rmse))

import random

class TweetGenerator:
  def __init__(self):
    self.templates = {  "announcement" : [ "ðŸš€ Exciting news from {company}! {message}",
                "Big announcement: {company} is {message} ðŸŽ‰",
                "Hey everyone! {company} has {message} âœ¨"],




                'question': [
                "What do you think about {topic}? Let us know! ðŸ’¬",
                "Quick question: How do you feel about {topic}? ðŸ¤”",
                "{company} wants to know: What's your take on {topic}? ðŸ—£ï¸"
            ],
            'general': [
                "Check out what {company} is up to! {message} ðŸŒŸ",
                "{company} update: {message} ðŸ’¯",
                "From the {company} team: {message} ðŸ”¥"
            ]
        }


  def random_generator(self,company,message,topic,tweet_type):
    if tweet_type == 'announcement':
      return random.choice(self.templates['announcement']).format(company=company, message=message)

    elif tweet_type == 'question':
      return random.choice(self.templates['question']).format(company=company, topic=topic)

    else:
      return random.choice(self.templates['general']).format(company=company, message=message)


#myclass = TweetGenerator()
#print(myclass.random_generator("nike","hello there","testing","general"))

from pyngrok import ngrok
ngrok.set_auth_token("2yEAHUJD5TiiTLspaGKcsXyP5Cx_385LqLih7PsvY3GJnfERf")

import random

class TweetGenerator:
  def __init__(self):
    self.templates = {  "announcement" : [ "ðŸš€ Exciting news from {company}! {message}",
                "Big announcement: {company} is {message} ðŸŽ‰",
                "Hey everyone! {company} has {message} âœ¨"],




                'question': [
                "What do you think about {topic}? Let us know! ðŸ’¬",
                "Quick question: How do you feel about {topic}? ðŸ¤”",
                "{company} wants to know: What's your take on {topic}? ðŸ—£ï¸"
            ],
            'general': [
                "Check out what {company} is up to! {message} ðŸŒŸ",
                "{company} update: {message} ðŸ’¯",
                "From the {company} team: {message} ðŸ”¥"
            ]
        }


  def generate_tweet(self,company,message,topic,tweet_type):
    if tweet_type == 'announcement':
      return random.choice(self.templates['announcement']).format(company=company, message=message)

    elif tweet_type == 'question':
      return random.choice(self.templates['question']).format(company=company, topic=topic)

    else:
      return random.choice(self.templates['general']).format(company=company, message=message)


#myclass = TweetGenerator()
#print(myclass.random_generator("nike","hello there","testing","general"))



# Install required packages (run this in a separate cell first)
!pip install flask pyngrok

from flask import Flask, request, jsonify
from pyngrok import ngrok
import threading


app = Flask(__name__)
generator = TweetGenerator()

@app.route('/generate', methods=['POST'])
def generate():
    try:
        data = request.get_json()

        # Get inputs
        company = data.get('company', 'Our Company')
        tweet_type = data.get('tweet_type', 'general')
        message = data.get('message', 'Something awesome!')
        topic = data.get('topic', 'innovation')

        # Generate tweet
        generated_tweet = generator.generate_tweet(company, tweet_type, message, topic)

        return jsonify({
            'generated_tweet': generated_tweet,
            'success': True,
            'company': company,
            'type': tweet_type
        })

    except Exception as e:
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'Tweet Generator API is running!'})

@app.route('/', methods=['GET'])
def home():
    return jsonify({'message': 'Tweet Generator API', 'endpoints': ['/health', '/generate']})

def run_flask():
    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)

# Start Flask in a separate thread
flask_thread = threading.Thread(target=run_flask)
flask_thread.daemon = True
flask_thread.start()

# Create ngrok tunnel
public_url = ngrok.connect(5000)
print(f"Public URL: {public_url}")
print(f"Health check: {public_url}/health")
print(f"Generate endpoint: {public_url}/generate")

# Keep the cell running
try:
    # This will keep the server running until you interrupt
    import time
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    print("Server stopped")
    ngrok.disconnect(public_url)

# bonus_ai_generator.py
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class AITweetGenerator:
    def __init__(self):
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        self.model = GPT2LMHeadModel.from_pretrained('gpt2')
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def generate_ai_tweet(self, prompt, max_length=60):
        inputs = self.tokenizer.encode(prompt, return_tensors='pt')

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                temperature=2.0,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        tweet = generated_text[len(prompt):].strip()
        return tweet[:280]  # Twitter limit

twitter = AITweetGenerator()
print(twitter.generate_ai_tweet("announcement   Exciting news from {nike}! {new shoes} "))

tweet = input()
inferred_company = input()   #CHANGE AFETRWARDS TO TAKE DIRECTLY FROM THE CLASS DEFINED ABOVE

def get_top_tfidf_word(row_index):
    # Get TF-IDF scores for this tweet
    scores = tfidf_matrix[row_index].toarray()[0]

    # Find index of highest score
    top_index = np.argmax(scores)

    # Return the word with highest score
    return feature_names[top_index]

def get_top_tfidf_word(text):
    # Transform text to TF-IDF vector
    transformed = vectorizer.transform([text])  # Convert single string to list

    # Get TF-IDF scores (same shape as original implementation)
    scores = transformed.toarray()[0]  # [0] extracts the first (only) document

    # Find index of highest score
    top_index = np.argmax(scores)

    # Return corresponding word from feature list
    return feature_names[top_index]




def hashtag(tweet):
  if "#" in tweet:
    return 1
  else:
    return 0

def has_media(tweet):
  if "pic.twitter.com" in tweet:
    return 1
  else:
    return 0


def word_count(tweet):
  return len(tweet.split())

def char_count(tweet):
  return len(tweet)

def sentiment(tweet):
  return TextBlob(tweet).sentiment.polarity

def company_avg_likes(tweet,company,company_to_avg_likes):
  return company_to_avg_likes[company]
  #see if the company is there in the dictionary if yes then return value of that otherwise return mean (dictionary values)


def avg_likes_for_keyword(tweet,keyword_to_avg_likes):

  return keyword_to_avg_likes[get_top_tfidf_word(tweet)]

  #find the keyword by tf idf function done above
  #if keyword not there in dictionary return mean(avg_likes_for_...)

  #else return the value from dictionary




print(hashtag(tweet))
print(has_media(tweet))
print(word_count(tweet))
print(char_count(tweet))
print(sentiment(tweet))
print(company_avg_likes(tweet,inferred_company,company_to_avg_likes))
print(avg_likes_for_keyword(tweet,keyword_to_avg_likes))
